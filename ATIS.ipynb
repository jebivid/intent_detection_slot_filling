{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATIS.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkxTynKh4D5xDRkKj9FyTs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGhKN00OU-IE"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/sz128/slot_filling_and_intent_detection_of_SLU.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!ls -lat"
      ],
      "metadata": {
        "id": "VGYDhXVHBZBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "'''\n",
        "A simple bidirectional lstm for joint slot filling and intent classification\n",
        "'''\n",
        "class IntentSlot(nn.Module):\n",
        "  def __init__(self, embedding_size, vocab_size, hidden_size, intent_size, slot_size):\n",
        "    super(IntentSlot, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
        "    self.lstm = nn.LSTM(input_size=embedding_size,\n",
        "                         hidden_size=hidden_size,\n",
        "                         num_layers=1,\n",
        "                         bidirectional=True)\n",
        "    print(intent_size, slot_size)\n",
        "    self.classifier_slot = nn.Linear(2*hidden_size, slot_size)\n",
        "    self.classifier_intent = nn.Linear(2*hidden_size, intent_size)\n",
        "  \n",
        "  def forward(self, x, mask):\n",
        "    x = self.embedding(x)\n",
        "    x, _ = self.lstm(x)\n",
        "    agg  = torch.sum(x * mask.unsqueeze(-1), dim=1)\n",
        "    agg = agg / (1e-9 + torch.sum(mask, dim=1, keepdim=True))\n",
        "    slots = self.classifier_slot(x)\n",
        "    intent = self.classifier_intent(agg)\n",
        "    return slots, intent\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   model = IntentSlot(100, 10, 10, 4, 2)\n",
        "   x = torch.tensor([[1, 4], [5, 6]])\n",
        "   mask = torch.tensor([[1, 1], [1, 1]])\n",
        "   slots, intent = model(x, mask)\n",
        "   print(slots.size(), intent.size())"
      ],
      "metadata": {
        "id": "OOX47KpNYGWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d812b2-e6c8-4819-b71d-13fe9b8eafe3"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 2\n",
            "torch.Size([2, 2, 2]) torch.Size([2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "'''\n",
        "create a tokenized dataset using the glove embeddings with a custom iterator\n",
        "'''\n",
        "class dataset:\n",
        "  def __init__(self, train_file, val_file, test_file, embedding_dim=100):\n",
        "    self.glove_vocab = self._build_glove_vocab(embedding_dim)\n",
        "    self.w_to_t, self.t_to_w, self.s_to_l, self.l_to_s, self.i_to_l, self.l_to_i \\\n",
        "     = {'<PAD>': 0, '<UNK>': 1}, {0: '<PAD>', 1: '<UNK>'}, {'<PAD>': 0}, {0: '<PAD>'} \\\n",
        "     , {}, {}\n",
        "    self._create_vocabulary(train_file, embedding_dim)\n",
        "    self._create_vocabulary(val_file, embedding_dim)\n",
        "    self._create_vocabulary(test_file, embedding_dim)\n",
        "    self.intent_size = len(self.i_to_l)\n",
        "    self.slot_size = len(self.s_to_l)\n",
        "    self.embedding_size = embedding_dim\n",
        "    self.vocab_size = len(self.t_to_w)\n",
        "    self.embedding = [np.zeros((1, embedding_dim)), np.random.randn(1, embedding_dim)]\n",
        "    for i in range(2, len(self.t_to_w)):\n",
        "      self.embedding.append(\n",
        "          self.glove_vocab[self.t_to_w[i]]\n",
        "          )\n",
        "    tokenized_train = self._process(train_file)\n",
        "    random.shuffle(tokenized_train)\n",
        "    tokenized_val = self._process(val_file)\n",
        "    tokenized_test = self._process(test_file)\n",
        "\n",
        "    self.splits = {'train': tokenized_train,\n",
        "                   'val': tokenized_val,\n",
        "                   'test': tokenized_test}\n",
        "\n",
        "  def iterate(self, mode, batch_size=64):\n",
        "    data = self.splits[mode]\n",
        "    idx = np.arange(len(data))\n",
        "    n_chunks = idx.shape[0] // batch_size + 1\n",
        "    for chunk_id, chunk in enumerate(np.array_split(idx, n_chunks)):\n",
        "        sentences = [torch.tensor(data[idx][0]) for idx in chunk]\n",
        "        slots = [torch.tensor(data[idx][1]) for idx in chunk]\n",
        "        intents = [torch.tensor(data[idx][2]).reshape(1) for idx in chunk]\n",
        "        padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=0)\n",
        "        padded_slots = pad_sequence(slots, batch_first=True, padding_value=0)\n",
        "        mask = (padded_slots != 0).float()\n",
        "        intents = torch.cat(intents, 0)\n",
        "        yield padded_sentences, padded_slots, intents, mask\n",
        "  \n",
        "  def _process(self, input_file):\n",
        "    with open(input_file, 'rt') as fi:\n",
        "      sentences = fi.read().strip().split('\\n')\n",
        "      tokenized = []\n",
        "      for i, sentence in enumerate(sentences):\n",
        "        text, intent = sentence.split(' <=> ')\n",
        "        tokens, slots = [], []\n",
        "        for word_slot in text.split(' '):\n",
        "            word, slot = word_slot.split(\":\")\n",
        "            if word not in self.w_to_t:\n",
        "              tokens.append(1)\n",
        "            else:\n",
        "              tokens.append(self.w_to_t[word])\n",
        "            slots.append(self.s_to_l[slot])\n",
        "        tokenized.append((tokens, slots, self.i_to_l[intent]))\n",
        "      return tokenized\n",
        "\n",
        "    \n",
        "  def _create_vocabulary(self, train_file, embedding_dim):\n",
        "    with open(train_file, 'rt') as fi:\n",
        "      sentences = fi.read().strip().split('\\n')\n",
        "      for sentence in sentences:\n",
        "        text, intent = sentence.split(' <=> ')\n",
        "        for idx, word_slot in enumerate(text.split(' ')):\n",
        "          word, slot = word_slot.split(\":\")\n",
        "          if slot not in self.s_to_l:\n",
        "            x = self.s_to_l[slot] = len(self.s_to_l)\n",
        "            self.l_to_s[x] = slot\n",
        "          if intent not in self.i_to_l:\n",
        "            x = self.i_to_l[intent] = len(self.i_to_l)\n",
        "            self.l_to_i[x] = intent\n",
        "          if word not in self.w_to_t and word in self.glove_vocab:\n",
        "            x = self.w_to_t[word] = len(self.w_to_t)\n",
        "            self.t_to_w[x] = word\n",
        "    \n",
        "\n",
        "  def _build_glove_vocab(self, dimension=100):\n",
        "    vocab = {}\n",
        "    with open('glove.6B.{}d.txt'.format(dimension),'rt') as fi:\n",
        "      full_content = fi.read().strip().split('\\n')\n",
        "      for line in full_content:\n",
        "        splits = line.split()\n",
        "        word, embedding = splits[0], list(map(float, splits[1:]))\n",
        "        vocab[word] = np.array(embedding)\n",
        "    print('finished reading glove', len(vocab))\n",
        "    return vocab\n",
        "    \n",
        "# if __name__ == '__main__':\n",
        "#   directory = 'slot_filling_and_intent_detection_of_SLU/data/atis-2/'\n",
        "#   d = dataset(directory+'train', directory+'valid', directory+'test')\n",
        "#   for x,y1,y2 in d.iterate('train'):\n",
        "#     print(x.size(), y1.size(), y2.size())"
      ],
      "metadata": {
        "id": "h5R_p9p6E8og"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def eval(model, mode='val'):\n",
        "  model.eval()\n",
        "  total_slot, total_intent, correct_slot, correct_intent = 0, 0, 0, 0\n",
        "  for x, y_s, y_i, mask in d.iterate(mode):\n",
        "    logits_slots, logits_intent = model(x, mask)\n",
        "    pred_slot = torch.argmax(logits_slots, dim=-1)\n",
        "    pred_intent = torch.argmax(logits_intent, dim=-1)\n",
        "    total_intent += y_i.size(0)\n",
        "    total_slot += torch.sum((y_s != 0).float()).item()\n",
        "    correct_slot += torch.sum(((pred_slot == y_s) & (y_s != 0)).float()).item()\n",
        "    correct_intent += torch.sum((pred_intent == y_i).float()).item()\n",
        "  return float(correct_slot) / total_slot, float(correct_intent) / total_intent \n",
        "\n",
        "directory = 'slot_filling_and_intent_detection_of_SLU/data/atis-2/'\n",
        "d = dataset(directory+'train', directory+'valid', directory+'test')\n",
        "model = IntentSlot(embedding_size=d.embedding_size, vocab_size=d.vocab_size, \n",
        "                   hidden_size = 512, intent_size=d.intent_size, slot_size=d.slot_size)\n",
        "loss_slot = nn.CrossEntropyLoss(ignore_index=0)\n",
        "loss_intent = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "eval_period, num_epochs, steps, best_acc = 20, 10, 0, 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for x, y_s, y_i, mask in d.iterate('train'):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    logits_slots, logits_intent = model(x, mask)\n",
        "    l = loss_intent(logits_intent, y_i)\n",
        "    l = l + loss_slot(logits_slots.view(-1, d.slot_size), y_s.view(-1))\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "    if steps % eval_period == 0:\n",
        "      slot_acc, intent_acc = eval(model, 'val')\n",
        "      print('val accuracy of slot filling is {0:.2f}  and accuracy of intent \\\n",
        "       classification is {1:.2f}'.format(slot_acc, intent_acc))\n",
        "      if slot_acc > best_acc:\n",
        "        best_model = copy.deepcopy(model)\n",
        "        best_acc = slot_acc\n",
        "      \n",
        "slot_acc, intent_acc = eval(best_model, 'test')\n",
        "print('test accuracy of slot filling is {0:.2f}  and accuracy of intent \\\n",
        "  classification is {1:.2f}'.format(slot_acc, intent_acc))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c7dVSD9vnRMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c934c9-cad4-4e86-88f6-31220f0db0e0"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished reading glove 400000\n",
            "26 128\n",
            "val accuracy of slot filling is 0.63  and accuracy of intent        classification is 0.71\n",
            "val accuracy of slot filling is 0.68  and accuracy of intent        classification is 0.71\n",
            "val accuracy of slot filling is 0.72  and accuracy of intent        classification is 0.72\n",
            "val accuracy of slot filling is 0.77  and accuracy of intent        classification is 0.77\n",
            "val accuracy of slot filling is 0.80  and accuracy of intent        classification is 0.77\n",
            "val accuracy of slot filling is 0.83  and accuracy of intent        classification is 0.80\n",
            "val accuracy of slot filling is 0.84  and accuracy of intent        classification is 0.80\n",
            "val accuracy of slot filling is 0.85  and accuracy of intent        classification is 0.79\n",
            "val accuracy of slot filling is 0.86  and accuracy of intent        classification is 0.83\n",
            "val accuracy of slot filling is 0.86  and accuracy of intent        classification is 0.84\n",
            "val accuracy of slot filling is 0.86  and accuracy of intent        classification is 0.86\n",
            "val accuracy of slot filling is 0.86  and accuracy of intent        classification is 0.85\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.86\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.87\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.87\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.88\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.88\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.89\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.88\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.90\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.89\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.90\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.90\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.90\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.91\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.90\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.91\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.91\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.90\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.91\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.90\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.91\n",
            "val accuracy of slot filling is 0.88  and accuracy of intent        classification is 0.91\n",
            "val accuracy of slot filling is 0.88  and accuracy of intent        classification is 0.92\n",
            "val accuracy of slot filling is 0.87  and accuracy of intent        classification is 0.91\n",
            "test accuracy of slot filling is 0.87  and accuracy of intent   classification is 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "NxRzpZccUTZN"
      }
    }
  ]
}